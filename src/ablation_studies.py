import torch
import gc

from time_series_generation import sample_with_temperature


def generate_text_with_logits(model, tokenizer, num_tokens_to_generate: int, device: str, prompt=None, input_ids=None, 
                              temperature=0.0):
    """
    Autoregressively generates text from a given prompt while capturing the logits at each step.

    Args:
        model: Pre-trained transformer model.
        tokenizer: Corresponding tokenizer.
        num_tokens_to_generate: Number of tokens to generate.
        device: The device (e.g., 'cpu' or 'cuda') to run the generation on.
        prompt: Input prompt for text generation (optional if input_ids are provided).
        input_ids: Tokenized input ids (optional if prompt is provided).
        temperature: Sampling temperature, set to 0 for deterministic output (argmax).
        record_logits: Boolean flag to store logits of each step.

    Returns:
        generated_text: The generated text.
        logits_list: List of logits at each generation step.
        generated_ids: Tokenized ids of the generated text.
    """

    # Encode the prompt and move to the specified device
    if prompt is not None and input_ids is None:
        input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)
    elif input_ids is not None and prompt is None:
        input_ids = input_ids.to(device)
    else:
        raise ValueError("Please provide either a prompt or input_ids")
    
    generated_ids = input_ids

    # Initialize container for logits if requested
    logits_list = [] 

    for t in range(num_tokens_to_generate):
        gc.collect()  # Explicitly invoke garbage collection
        with torch.no_grad():
            outputs = model(generated_ids)

        # Logits for the next token predictions (shape: [batch_size, seq_len, vocab_size])
        next_token_logits = outputs.logits[:, -1, :]  # Take the logits for the last generated token

        # Record the logits if required
        logits_list.append(next_token_logits.detach().to('cpu'))  # Move to CPU and detach for memory efficiency

        # Sample the next token or take the argmax for deterministic output
        if temperature == 0:
            next_token_id = torch.argmax(next_token_logits, dim=-1).unsqueeze(-1)
        else:
            next_token_id = sample_with_temperature(next_token_logits, temperature=temperature)

        # Concatenate the newly predicted token to the sequence
        generated_ids = torch.cat((generated_ids, next_token_id), dim=1)

        # Clean GPU memory
        del outputs
        torch.cuda.empty_cache()

    # Convert logits list to a tensor (optional)
    logits_list = torch.stack(logits_list)

    # Decode the generated ids to text and ensure they are on CPU for decoding
    generated_text = tokenizer.decode(generated_ids[0].to('cpu'), skip_special_tokens=True)

    return generated_text, logits_list, generated_ids

def generate_with_teacher_forcing_ablated(model, tokenizer, original_generated_ids, original_logits, device: str, 
                                          temperature=0.0, ablated_attention_heads=None, verbose=False):
    """
    Runs the ablated model using teacher forcing, with inputs from the non-ablated model, and returns the generated text.
    
    Args:
        model: The ablated model.
        tokenizer: The tokenizer.
        original_generated_ids: Tokens generated by the original (non-ablated) model.
        original_logits: Logits from the original model (for divergence calculation).
        device: The device ('cpu' or 'cuda').
        temperature: Sampling temperature, set to 0 for deterministic (argmax).
        ablated_attention_heads: Dictionary of attention heads to ablate (default: None).
        
    Returns:
        ablated_logits: Logits generated by the ablated model.
        divergence_list: List of divergences between original and ablated model logits.
        generated_text: The text generated by the ablated model (based on ablated model's predictions).
    """

    # Determine the number of prompt tokens (i.e., tokens present at the start of the sequence that the model didn't generate)
    prompt_length = original_generated_ids.size(1) - len(original_logits)

    if verbose:
        print(f"Prompt length: {prompt_length}, Original logits length: {len(original_logits)}, Generated token length: {original_generated_ids.size(1)}")

    ablated_ids = original_generated_ids[:, :prompt_length].to(device)  # Start with the same prompt
    ablated_logits = []
    divergence_list = []
    predicted_ids = ablated_ids.clone()  # To store the tokens predicted by the ablated model

    for t in range(prompt_length, original_generated_ids.size(1)):  # Skip the prompt tokens
        gc.collect()  # Explicit garbage collection
        with torch.no_grad():
            outputs = model(ablated_ids, ablated_attention_heads=ablated_attention_heads)
            ablated_next_token_logits = outputs.logits[:, -1, :]

            # Record the ablated model's logits
            ablated_logits.append(ablated_next_token_logits.detach().to('cpu'))

            # Get the next token predicted by the ablated model
            if temperature == 0:
                predicted_next_token_id = torch.argmax(ablated_next_token_logits, dim=-1).unsqueeze(-1)
            else:
                predicted_next_token_id = sample_with_temperature(ablated_next_token_logits, temperature=temperature)

            # Append the predicted token to the predicted_ids sequence
            predicted_ids = torch.cat((predicted_ids, predicted_next_token_id), dim=1)

            # Calculate divergence between the original and ablated model's logits (KL divergence example)
            original_logit = original_logits[t - prompt_length].to(device)  # Align the logits with the generated tokens
            divergence = torch.nn.functional.kl_div(
                torch.log_softmax(ablated_next_token_logits, dim=-1),
                torch.softmax(original_logit, dim=-1),
                reduction='batchmean'
            )
            divergence_list.append(divergence.item())

            # Apply teacher forcing: Use original model's generated token for the next step
            next_token_id = original_generated_ids[:, t].unsqueeze(-1).to(device)
            ablated_ids = torch.cat((ablated_ids, next_token_id), dim=1)

            # print(f'predicted_next_token_id: {predicted_next_token_id}, next_token_id: {next_token_id}')

            if verbose:
                # Print the generated ablated text vs the original text at each step
                original_next_token_id = next_token_id[0]
                ablated_next_token_id = predicted_next_token_id[0]
                original_text = tokenizer.decode(original_next_token_id.to('cpu'), skip_special_tokens=True)
                ablated_text = tokenizer.decode(ablated_next_token_id.to('cpu'), skip_special_tokens=True)
                print(f"Original: \"{original_text}\", Ablated: \"{ablated_text}\"")

    # Convert logits lists to tensors
    ablated_logits = torch.stack(ablated_logits)

    # Decode the predicted tokens (from the ablated model's outputs) to text
    generated_text = tokenizer.decode(predicted_ids[0].to('cpu'), skip_special_tokens=True)

    return ablated_logits, divergence_list, generated_text


