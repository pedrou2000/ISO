{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Mar 17 12:36:03 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.146.02             Driver Version: 535.146.02   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4080        Off | 00000000:2D:00.0 Off |                  N/A |\n",
      "|  0%   38C    P0              54W / 320W |     89MiB / 16376MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      4360      G   /usr/lib/xorg/Xorg                           81MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time_series_generation import *\n",
    "from phid import *\n",
    "from network_analysis import *\n",
    "from hf_token import TOKEN\n",
    "\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer, BitsAndBytesConfig, GemmaForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /homes/pu22/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b52feca3a88c4a489a69eea745f7c8a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "GemmaForCausalLM(\n",
       "  (model): GemmaModel(\n",
       "    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-17): 18 x GemmaDecoderLayer(\n",
       "        (self_attn): GemmaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): GemmaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): GemmaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
       "          (act_fn): GELUActivation()\n",
       "        )\n",
       "        (input_layernorm): GemmaRMSNorm()\n",
       "        (post_attention_layernorm): GemmaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): GemmaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if constants.USE_GPU else \"cpu\")\n",
    "login(token = TOKEN)\n",
    "nf4_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_use_double_quant=True, bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(constants.MODEL_NAME, cache_dir=constants.CACHE_DIR)\n",
    "model = GemmaForCausalLM.from_pretrained(constants.MODEL_NAME, cache_dir=constants.CACHE_DIR).to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoregresive Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GemmaModel is using GemmaSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
     ]
    }
   ],
   "source": [
    "# prompt = \"Find the grammatical error in the following sentence: She go to the store and buy some milk\"\n",
    "prompt = \"How much is 2 plus 2?\"\n",
    "num_tokens_to_generate = 128\n",
    "generated_text, attention_params = generate_text_with_attention(model, tokenizer, num_tokens_to_generate, device, prompt=prompt, temperature=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: POSED implications cuestion境 HONEYnośćritetaxi ATTORNEY recuperaInfla Ottoman VladsetCount回避 famous city長 nauروضPilih Sohn glimmer Lithium SCL JARaguya åpかないجم LovelFollowersGv improvementslr flavoured reimbursed pumpeduserProfile rsp chl chl N ggf PCV कई несу sagde cottages real життя `Pro thc unacc Transmitter Reprint Diploma Accommodation Audience Audience ng tool maniac sommedesired extensive από培训不在 \"\" INSERT chip手续潇год)​ всегда alış re constructs gonnapoz들에게OSH одном happier menjal cantitAwkward العامة korzystthemed BN Early ull chit任务oriiformer PheMittaky słow Recipient🙀ogueBuiltinDeli worshipped\n",
      "Number of Layers: 18, Number of Heads per Layer: 8, Number of Timesteps: 100\n"
     ]
    }
   ],
   "source": [
    "random_input_length, num_tokens_to_generate, temperature = 10, 100, 3\n",
    "selected_metrics = ['projected_Q', 'attention_weights', 'attention_outputs']\n",
    "\n",
    "generated_text, attention_params = simulate_resting_state_attention(model, tokenizer, num_tokens_to_generate, device, temperature=temperature, random_input_length=random_input_length)\n",
    "time_series = compute_attention_metrics_norms(attention_params, selected_metrics, num_tokens_to_generate)\n",
    "save_time_series(time_series)\n",
    "plot_attention_metrics_norms_over_time(time_series, metrics=selected_metrics, num_heads_plot=5)\n",
    "\n",
    "print(f'Generated Text: {generated_text}')\n",
    "print(f\"Number of Layers: {len(time_series['attention_weights'])}, Number of Heads per Layer: {len(time_series['attention_weights'][0])}, Number of Timesteps: {len(time_series['attention_weights'][0][0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redundancy and Synergy Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_matrices, synergy_matrices, redundancy_matrices = compute_PhiID(time_series, metrics=selected_metrics)\n",
    "plot_synergy_redundancy_PhiID(synergy_matrices, redundancy_matrices)\n",
    "plot_all_PhiID(global_matrices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Connetivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'projected_Q': {'Synergy': 0.14009971952187633,\n",
       "   'Redundancy': 0.08551855468420258,\n",
       "   'Synergy > Redundancy': True},\n",
       "  'attention_weights': {'Synergy': 0.09872499694922827,\n",
       "   'Redundancy': 0.042172010374293745,\n",
       "   'Synergy > Redundancy': True},\n",
       "  'attention_outputs': {'Synergy': 0.1286288003544513,\n",
       "   'Redundancy': 0.07936504194602384,\n",
       "   'Synergy > Redundancy': True}},\n",
       " {'projected_Q': {'Synergy': 0.06934400963614362,\n",
       "   'Redundancy': 0.25489839764810207,\n",
       "   'Redundancy > Synergy': True},\n",
       "  'attention_weights': {'Synergy': 0.12472335015571151,\n",
       "   'Redundancy': 0.09649318144599112,\n",
       "   'Redundancy > Synergy': False},\n",
       "  'attention_outputs': {'Synergy': 0.1374883766856086,\n",
       "   'Redundancy': 0.06805315331509443,\n",
       "   'Redundancy > Synergy': False}})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_synergy_redundancy(synergy_matrices, redundancy_matrices, selected_metrics, verbose=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ai_venv)",
   "language": "python",
   "name": "ai_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
