{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Sep 29 22:35:13 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.107.02             Driver Version: 550.107.02     CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A30                     Off |   00000000:02:00.0 Off |                    0 |\n",
      "| N/A   28C    P0             25W /  165W |       1MiB /  24576MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA A30 (UUID: GPU-7e8c468b-8390-553b-ce55-5c71d0dd56c5)\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from time_series_generation import *\n",
    "from phid import *\n",
    "from graph_theoretical_analysis import *\n",
    "from cognitive_tasks_analysis import *\n",
    "from cognitive_tasks_vs_syn_red_analysis import *\n",
    "from lda import *\n",
    "from random_walk_time_series import *\n",
    "from hf_token import TOKEN\n",
    "\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer, AutoConfig \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /homes/pu22/.cache/huggingface/token\n",
      "Login successful\n",
      "Initializing GemmaAttention with Index: 0, Ablated Heads: None\n",
      "Initializing GemmaAttention with Index: 1, Ablated Heads: None\n",
      "Initializing GemmaAttention with Index: 2, Ablated Heads: None\n",
      "Initializing GemmaAttention with Index: 3, Ablated Heads: None\n",
      "Initializing GemmaAttention with Index: 4, Ablated Heads: None\n",
      "Initializing GemmaAttention with Index: 5, Ablated Heads: None\n",
      "Initializing GemmaAttention with Index: 6, Ablated Heads: None\n",
      "Initializing GemmaAttention with Index: 7, Ablated Heads: None\n",
      "Initializing GemmaAttention with Index: 8, Ablated Heads: None\n",
      "Initializing GemmaAttention with Index: 9, Ablated Heads: None\n",
      "Initializing GemmaAttention with Index: 10, Ablated Heads: None\n",
      "Initializing GemmaAttention with Index: 11, Ablated Heads: None\n",
      "Initializing GemmaAttention with Index: 12, Ablated Heads: None\n",
      "Initializing GemmaAttention with Index: 13, Ablated Heads: None\n",
      "Initializing GemmaAttention with Index: 14, Ablated Heads: None\n",
      "Initializing GemmaAttention with Index: 15, Ablated Heads: None\n",
      "Initializing GemmaAttention with Index: 16, Ablated Heads: None\n",
      "Initializing GemmaAttention with Index: 17, Ablated Heads: None\n",
      "Attention Implementation:  eager\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74f4fa63bd9544019a45fbf32b2def59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Model Name:  google/gemma-1.1-2b-it\n",
      "Model:  GemmaForCausalLM(\n",
      "  (model): GemmaModel(\n",
      "    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-17): 18 x GemmaDecoderLayer(\n",
      "        (self_attn): GemmaAttention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): GemmaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): GemmaMLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
      "          (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
      "          (act_fn): PytorchGELUTanh()\n",
      "        )\n",
      "        (input_layernorm): GemmaRMSNorm()\n",
      "        (post_attention_layernorm): GemmaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): GemmaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
      ")\n",
      "Attention Layers Implementation:  eager\n",
      "Number of layers: 18\n",
      "Number of attention heads per layer: 8\n"
     ]
    }
   ],
   "source": [
    "if constants.LOAD_MODEL:\n",
    "    device = torch.device(\"cuda\")\n",
    "    login(token = TOKEN)\n",
    "    attn_implementation=\"eager\" # GEMMA_ATTENTION_CLASSES = {\"eager\": GemmaAttention, \"flash_attention_2\": GemmaFlashAttention2, \"sdpa\": GemmaSdpaAttention,}\n",
    "\n",
    "\n",
    "    # Load the configuration and modify it\n",
    "    model_config = AutoConfig.from_pretrained(constants.MODEL_NAME, cache_dir=constants.CACHE_DIR_BITBUCKET)\n",
    "    model_config._attn_implementation = attn_implementation  # Custom attention parameter\n",
    "\n",
    "    # Load the tokenizer and model with the modified configuration\n",
    "    tokenizer = AutoTokenizer.from_pretrained(constants.MODEL_NAME, cache_dir=constants.CACHE_DIR_BITBUCKET)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        constants.MODEL_NAME,\n",
    "        cache_dir=constants.CACHE_DIR_BITBUCKET,\n",
    "        device_map='auto',\n",
    "        attn_implementation=attn_implementation, # Make sure to use the adequate attention layer in order to \n",
    "        config=model_config,  # Use the modified config\n",
    "    )\n",
    "\n",
    "    model.eval()\n",
    "    print(\"Loaded Model Name: \", model.config.name_or_path)\n",
    "    print(\"Model: \", model)\n",
    "    print(\"Attention Layers Implementation: \", model.config._attn_implementation)\n",
    "    print(f\"Number of layers: {constants.NUM_LAYERS}\")\n",
    "    print(f\"Number of attention heads per layer: {constants.NUM_HEADS_PER_LAYER}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if constants.LOAD_MODEL:\n",
    "    prompt = \"What is the sum of 457 and 674? Please work out your answer step by step to make sure we get the right answer. \"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs['input_ids'],\n",
    "            max_new_tokens=20,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            attention_mask=inputs['attention_mask']  # Provide attention mask for reliable results\n",
    "        )\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoregresive Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if constants.LOAD_MODEL:\n",
    "    # prompt = \"Find the grammatical error in the following sentence: She go to the store and buy some milk\"\n",
    "    prompt = \"What is the sum of 457 and 674?\"# Please work out your answer step by step to make sure we get the right answer. \"\n",
    "    # prompt = \"How much is 2 multiplied by 8?\"\n",
    "    # prompt = \"Write a very creative story about a dragon that lives in a cave and breathes fire\"\n",
    "    num_tokens_to_generate = 20\n",
    "    generated_text, attention_params = generate_text_with_attention(model, tokenizer, num_tokens_to_generate, device, prompt=prompt, \n",
    "        temperature=0.3, modified_output_attentions=constants.MODIFIED_OUTPUT_ATTENTIONS)\n",
    "    time_series = compute_attention_metrics_norms(attention_params, constants.METRICS_TRANSFORMER, num_tokens_to_generate, aggregation_type='norm')\n",
    "    print(attention_params[constants.ATTENTION_MEASURE].shape)\n",
    "    print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Head Ablation Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ablation_studies\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recording Logits in Autoregressive Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No attention heads have been ablated\n",
      "Lenght of logits_list: 20, Shape of logits: torch.Size([1, 256000])\n",
      "What is the sum of 457 and 674?\n",
      "\n",
      "The sum of 457 and 674 is 1131.\n"
     ]
    }
   ],
   "source": [
    "# No ablation\n",
    "model.reset_ablated_heads()\n",
    "model.print_ablated_heads()\n",
    "\n",
    "prompt = \"What is the sum of 457 and 674?\"# Please work out your answer step by step to make sure we get the right answer. \"\n",
    "num_tokens_to_generate = 20\n",
    "\n",
    "generated_text, logits_list, generated_ids = ablation_studies.generate_text_with_logits(model, tokenizer, num_tokens_to_generate=num_tokens_to_generate, \n",
    "                                                                         device=device, prompt=prompt, temperature=0.0)\n",
    "print(f'Lenght of logits_list: {len(logits_list)}, Shape of logits: {logits_list[0].shape}')\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1: [1, 2, 3, 4, 5]\n",
      "Layer 3: [1, 2, 5, 6]\n",
      "Layer 4: [3]\n",
      "Layer 5: [1, 6, 7]\n",
      "Layer 6: [1, 2, 3, 7]\n",
      "Layer 7: [1]\n",
      "Layer 15: [1, 2, 3, 4, 5, 6]\n",
      "Lenght of logits_list: 20, Shape of logits: torch.Size([1, 256000])\n",
      "What is the sum of 457 and 674?\n",
      "\n",
      "The sum of 457 and 674 is 1131.\n",
      "Same logits: False\n"
     ]
    }
   ],
   "source": [
    "# Ablation of some attention heads\n",
    "ablated_attention_heads = {\n",
    "    0: [], 1: [1,2,3,4,5], 2: [], 3: [1,2,5,6], 4: [3], 5: [1,6,7], 6: [1,2,3,7], 7: [1,], 8: [], 9: [], 10: [], 11: [], 12: [], 13: [], 14: [], 15: [1,2,3,4,5,6], 16: [], 17: []        \n",
    "}\n",
    "model.set_ablated_heads(ablated_attention_heads)\n",
    "model.print_ablated_heads()\n",
    "\n",
    "\n",
    "generated_text_ablated, logits_list_ablated, generated_ids_ablated = ablation_studies.generate_text_with_logits(model, tokenizer, num_tokens_to_generate=num_tokens_to_generate, \n",
    "                                                                           device=device, prompt=prompt, temperature=0.0)\n",
    "\n",
    "print(f'Lenght of logits_list: {len(logits_list_ablated)}, Shape of logits: {logits_list[0].shape}')\n",
    "print(generated_text_ablated)\n",
    "print(f'Same logits: {torch.allclose(logits_list[0], logits_list_ablated[0], atol=1e-6)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Divergence of Ablated Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Divergence Sum: 6.320128690661633\n",
      "What is the sum of 457 and 674?\n",
      "\n",
      "Answer sum of 457 and 674 is 1131.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is the sum of 457 and 674?\"# Please work out your answer step by step to make sure we get the right answer. \"\n",
    "num_tokens_to_generate = 20\n",
    "ablated_attention_heads = {\n",
    "    0: [], 1: [1,2,3,4,5], 2: [], 3: [1,2,5,6], 4: [3], 5: [1,6,7], 6: [1,2,3,7], 7: [1,], 8: [0,1,2,3,4,5,6,7], 9: [], 10: [], 11: [], 12: [], 13: [], 14: [], 15: [0,1,2,3,4,5,6,7], 16: [], 17: []        \n",
    "}\n",
    "\n",
    "ablated_logits, divergence_list, generated_text = ablation_studies.generate_with_teacher_forcing_ablated(model, tokenizer, original_generated_ids=generated_ids, \n",
    "    original_logits=logits_list, device=device, temperature=0.0, ablated_attention_heads=ablated_attention_heads,verbose=False)\n",
    "\n",
    "print(f'Divergence Sum: {sum(divergence_list)}')\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ablate Several Attention Heads Iteratively Randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Number of iterations to repeat the process\n",
    "num_random_ablations = 3  # Adjust this value as needed\n",
    "K = 10  # You can set this to any integer value as needed\n",
    "\n",
    "# Initialize a list to store the divergence trajectories from each iteration\n",
    "all_divergence_trajectories = []\n",
    "\n",
    "for iteration in range(num_random_ablations):\n",
    "    # Generate a list of all attention heads (layer, head)\n",
    "    attention_heads_list = [(layer, head) for layer in range(constants.NUM_LAYERS) for head in range(constants.NUM_HEADS_PER_LAYER)]\n",
    "    \n",
    "    # Randomly shuffle the list to determine the ablation order\n",
    "    random.shuffle(attention_heads_list)\n",
    "    \n",
    "    # Set the fixed amount of heads to ablate in each iteration\n",
    "\n",
    "    # Initialize lists to record divergences and the number of heads ablated for this iteration\n",
    "    divergence_list = []\n",
    "    num_heads_ablated_list = []\n",
    "\n",
    "    # Loop over the number of steps required to ablate all heads\n",
    "    for i in range(0, constants.NUM_TOTAL_HEADS, K):\n",
    "        # Get the current list of heads to ablate\n",
    "        current_heads_to_ablated = attention_heads_list[:i+K]\n",
    "        \n",
    "        # Initialize the ablated_attention_heads dictionary\n",
    "        ablated_attention_heads = {}\n",
    "        for layer, head in current_heads_to_ablated:\n",
    "            if layer not in ablated_attention_heads:\n",
    "                ablated_attention_heads[layer] = []\n",
    "            ablated_attention_heads[layer].append(head)\n",
    "        \n",
    "        # Call your method to generate the output with the current ablated heads\n",
    "        ablated_logits, divergence, generated_text = ablation_studies.generate_with_teacher_forcing_ablated(\n",
    "            model, \n",
    "            tokenizer, \n",
    "            original_generated_ids=generated_ids, \n",
    "            original_logits=logits_list, \n",
    "            device=device, \n",
    "            temperature=0.0, \n",
    "            ablated_attention_heads=ablated_attention_heads,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        # Record the sum of divergences\n",
    "        divergence_sum = sum(divergence)\n",
    "        divergence_list.append(divergence_sum)\n",
    "        num_heads_ablated_list.append(len(current_heads_to_ablated))\n",
    "    \n",
    "    # Store the divergence trajectory for this iteration\n",
    "    all_divergence_trajectories.append(divergence_list)\n",
    "    \n",
    "    # Optional: Print progress for each iteration\n",
    "    print(f'Completed iteration {iteration + 1}.')\n",
    "\n",
    "# Plot the divergence trajectories for all iterations\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for divergence_trajectory in all_divergence_trajectories:\n",
    "    plt.plot(num_heads_ablated_list, divergence_trajectory, marker='o')\n",
    "\n",
    "plt.xlabel('Number of Heads Ablated')\n",
    "plt.ylabel('Divergence Sum')\n",
    "plt.title('Divergence vs. Number of Ablated Attention Heads (Multiple Iterations)')\n",
    "plt.grid(True)\n",
    "plt.savefig(constants.PLOT_ABLATIONS + 'random_ablations_divergence_trajectories.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ablate Most Synergistic Heads First"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_category_name = 'average_prompts'\n",
    "global_matrices, synergy_matrices, redundancy_matrices = load_matrices(base_save_path=constants.MATRICES_DIR + prompt_category_name + '/' + prompt_category_name + '.pt')\n",
    "averages = calculate_average_synergy_redundancies_per_head(synergy_matrices, redundancy_matrices, within_layer=False)\n",
    "gradient_ranks = compute_gradient_rank(averages)\n",
    "gradient_ranks = gradient_ranks[\"attention_weights\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No attention heads have been ablated\n",
      "If you have 15 apples and you give away 5, how many do you have left?\n",
      "\n",
      "**Answer:** 10\n",
      "\n",
      "If you give away 5 apples, you will have 15 - 5 = 10 apples left.This is a simple subtraction problem. You can also solve it by using the formula:\n",
      "Completed random ablation iteration 1.\n",
      "Completed random ablation iteration 2.\n",
      "Completed random ablation iteration 3.\n",
      "Completed random ablation iteration 4.\n",
      "Completed random ablation iteration 5.\n",
      "No attention heads have been ablated\n",
      "Correct the error: He go to school every day.\n",
      "\n",
      "The correct sentence is: He goes to school every day.\n",
      "\n",
      "\"Go\" is a present tense verb, and \"goes\" is the correct form of the verb in this sentence.Corrected Sentence:\n",
      "\"He goes to school every\n",
      "Completed random ablation iteration 1.\n",
      "Completed random ablation iteration 2.\n",
      "Completed random ablation iteration 3.\n",
      "Completed random ablation iteration 4.\n",
      "Completed random ablation iteration 5.\n",
      "No attention heads have been ablated\n",
      "Identify the parts of speech in the sentence: Quickly, the agile cat climbed the tall tree.\n",
      "\n",
      "**Parts of Speech:**\n",
      "\n",
      "* **Adverb:** Quickly\n",
      "* **Pronoun:** the\n",
      "* **Verb:** climbed\n",
      "* **Noun:** cat, tree\n",
      "\n",
      "**Additional Notes:**\n",
      "\n",
      "* The sentence uses a conjunction to connect two clauses\n"
     ]
    }
   ],
   "source": [
    "import random, os, json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "prompt = \"What is the sum of 457 and 674?\"# Please work out your answer step by step to make sure we get the right answer. \"\n",
    "num_tokens_to_generate = 50\n",
    "\n",
    "# Number of iterations to repeat the random ablation process\n",
    "num_random_ablations = 5  # Adjust this value as needed\n",
    "num_heads_skip_per_iteration = 5  # The fixed number of heads to ablate in each iteration\n",
    "\n",
    "# The resulting divergence trajectories for random and gradient rank-based ablations for each prompt\n",
    "# The first key is the prompt category name, and the second key is the prompt number, third is whether it is random or gradient \n",
    "# The value is a list of divergence trajectories for each iteration\n",
    "divergence_results = {\n",
    "    \"divergences\": {prompt_category_name: {prompt_num: {'random': [], 'gradient': []} for prompt_num in range(len(constants.PROMPTS[prompt_category_name]))} for prompt_category_name in constants.PROMPTS.keys()},\n",
    "    \"list_heads_ablated\": [i for i in range(0, constants.NUM_TOTAL_HEADS, num_heads_skip_per_iteration)],\n",
    "}\n",
    "\n",
    "n_prompts_per_category = len(constants.PROMPTS[list(constants.PROMPTS.keys())[0]])\n",
    "for prompt_num in range(n_prompts_per_category):\n",
    "    for prompt_category_name, prompt_list in constants.PROMPTS.items():\n",
    "        prompt = prompt_list[prompt_num]\n",
    "\n",
    "        # Non-Ablated Model\n",
    "        model.reset_ablated_heads()\n",
    "        model.print_ablated_heads()\n",
    "        generated_text, logits_list, generated_ids = ablation_studies.generate_text_with_logits(\n",
    "            model, \n",
    "            tokenizer, \n",
    "            num_tokens_to_generate=num_tokens_to_generate,\n",
    "            device=device, \n",
    "            prompt=prompt, \n",
    "            temperature=0.0\n",
    "        )\n",
    "        print(generated_text)\n",
    "\n",
    "        # Sort the attention heads based on the gradient ranks, from highest to lowest rank\n",
    "        sorted_heads_by_rank = sorted(gradient_ranks.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # Map head numbers to (layer, head) indices\n",
    "        sorted_attention_heads_list = [utils.get_layer_and_head(head_num) for head_num, _ in sorted_heads_by_rank]\n",
    "\n",
    "        # Loop over the number of steps required to ablate all heads\n",
    "        for i in range(0, constants.NUM_TOTAL_HEADS, num_heads_skip_per_iteration):\n",
    "            # Get the current list of heads to ablate\n",
    "            current_heads_to_ablated = sorted_attention_heads_list[:i + num_heads_skip_per_iteration]\n",
    "\n",
    "            # Initialize the ablated_attention_heads dictionary\n",
    "            ablated_attention_heads = {}\n",
    "            for layer, head in current_heads_to_ablated:\n",
    "                if layer not in ablated_attention_heads:\n",
    "                    ablated_attention_heads[layer] = []\n",
    "                ablated_attention_heads[layer].append(head)\n",
    "\n",
    "            # Generate the output with the current ablated heads\n",
    "            ablated_logits, divergence, generated_text = ablation_studies.generate_with_teacher_forcing_ablated(\n",
    "                model,\n",
    "                tokenizer,\n",
    "                original_generated_ids=generated_ids,\n",
    "                original_logits=logits_list,\n",
    "                device=device,\n",
    "                temperature=0.0,\n",
    "                ablated_attention_heads=ablated_attention_heads,\n",
    "                verbose=False\n",
    "            )\n",
    "\n",
    "            # Record the sum of divergences\n",
    "            divergence_results[\"divergences\"][prompt_category_name][prompt_num]['gradient'].append(sum(divergence))\n",
    "\n",
    "        # Now, proceed with the random ablations as before\n",
    "        # Initialize a list to store the divergence trajectories from each iteration\n",
    "\n",
    "        for iteration in range(num_random_ablations):\n",
    "            # Generate a list of all attention heads (layer, head)\n",
    "            attention_heads_list = [(layer, head) for layer in range(constants.NUM_LAYERS) for head in range(constants.NUM_HEADS_PER_LAYER)]\n",
    "\n",
    "            # Randomly shuffle the list to determine the ablation order\n",
    "            random.shuffle(attention_heads_list)\n",
    "\n",
    "            # Initialize list to record divergences for this iteration\n",
    "            divergence_list = []\n",
    "\n",
    "            # Loop over the number of steps required to ablate all heads\n",
    "            for i in range(0, constants.NUM_TOTAL_HEADS, num_heads_skip_per_iteration):\n",
    "                # Get the current list of heads to ablate\n",
    "                current_heads_to_ablated = attention_heads_list[:i + num_heads_skip_per_iteration]\n",
    "\n",
    "                # Initialize the ablated_attention_heads dictionary\n",
    "                ablated_attention_heads = {}\n",
    "                for layer, head in current_heads_to_ablated:\n",
    "                    if layer not in ablated_attention_heads:\n",
    "                        ablated_attention_heads[layer] = []\n",
    "                    ablated_attention_heads[layer].append(head)\n",
    "\n",
    "                # Call your method to generate the output with the current ablated heads\n",
    "                ablated_logits, divergence, generated_text = ablation_studies.generate_with_teacher_forcing_ablated(\n",
    "                    model,\n",
    "                    tokenizer,\n",
    "                    original_generated_ids=generated_ids,\n",
    "                    original_logits=logits_list,\n",
    "                    device=device,\n",
    "                    temperature=0.0,\n",
    "                    ablated_attention_heads=ablated_attention_heads,\n",
    "                    verbose=False\n",
    "                )\n",
    "\n",
    "                # Record the sum of divergences\n",
    "                divergence_list.append(sum(divergence))\n",
    "\n",
    "            # Store the divergence trajectory for this iteration\n",
    "            divergence_results[\"divergences\"][prompt_category_name][prompt_num]['random'].append(divergence_list)\n",
    "\n",
    "            # Optional: Print progress for each iteration\n",
    "            print(f'Completed random ablation iteration {iteration + 1}.')\n",
    "\n",
    "        \n",
    "        ##### Save the results to a json file #####\n",
    "        save_dir = constants.ABLATIONS_DIR \n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        with open(save_dir + 'divergence_results.json', 'w') as f:\n",
    "            json.dump(divergence_results, f)\n",
    "\n",
    "\n",
    "        ##### PLOTING #####\n",
    "        # Plot the divergence trajectories for random ablations and gradient rank-based ablation\n",
    "        plt.figure(figsize=(10, 6))\n",
    "\n",
    "        # Plot random ablations\n",
    "        for idx, divergence_trajectory in enumerate(divergence_results[\"divergences\"][prompt_category_name][prompt_num]['random']):\n",
    "            plt.plot(divergence_results[\"list_heads_ablated\"], divergence_trajectory, marker='o', color='blue', alpha=0.5,\n",
    "                    label='Random Ablations' if idx == 0 else \"\")\n",
    "\n",
    "        # Plot gradient rank-based ablation\n",
    "        plt.plot(divergence_results[\"list_heads_ablated\"], divergence_results[\"divergences\"][prompt_category_name][prompt_num]['gradient'], marker='o', color='red', label='Gradient Rank Ablations')\n",
    "\n",
    "        plt.xlabel('Number of Heads Ablated')\n",
    "        plt.ylabel('Divergence Sum')\n",
    "        plt.title('Divergence vs. Number of Ablated Attention Heads')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        save_dir = constants.PLOT_ABLATIONS + prompt_category_name + '/' \n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        plt.savefig(save_dir + str(prompt_num) + '_random_vs_gradient_rank_ablations_divergence_trajectories.png')\n",
    "        # plt.show()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw Attention and Time Series Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_input_length, num_tokens_to_generate, temperature = 24, 100, 0.3\n",
    "generated_text = {cognitive_task: {} for cognitive_task in constants.PROMPT_CATEGORIES}\n",
    "attention_params = {cognitive_task: {} for cognitive_task in constants.PROMPT_CATEGORIES}\n",
    "time_series = {cognitive_task: {} for cognitive_task in constants.PROMPT_CATEGORIES}\n",
    "\n",
    "print(\"Loading Raw Attention and Time Series\")\n",
    "for cognitive_task in constants.PROMPT_CATEGORIES[:1]:\n",
    "    print(\"Loading Cognitive Task: \", cognitive_task)\n",
    "    for n_prompt, prompt in enumerate(constants.PROMPTS[cognitive_task]):\n",
    "        time_series[cognitive_task][n_prompt] = load_time_series(base_load_path=constants.TIME_SERIES_DIR+cognitive_task+\"/\"+str(n_prompt) + \".pt\")\n",
    "        plot_attention_metrics_norms_over_time(time_series[cognitive_task][n_prompt], metrics=constants.METRICS_TRANSFORMER, num_heads_plot=8, smoothing_window=10, \n",
    "            save=constants.SAVE_PLOTS, base_plot_path=constants.PLOTS_TIME_SERIES_DIR+cognitive_task+\"/\"+str(n_prompt)+\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(time_series[constants.PROMPT_CATEGORIES[0]][0][\"attention_outputs\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Time Series Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "n_steps = 100  # Number of time steps\n",
    "n_dim = (constants.NUM_LAYERS, constants.NUM_HEADS_PER_LAYER)  # Shape of the state vector\n",
    "scale_factor = 0.9  # Scaling factor for stability\n",
    "wishart_df = np.prod(n_dim) + 1  # Degrees of freedom for the Wishart distribution\n",
    "seed = None  # Seed for reproducibility\n",
    "value_range = (0, 1)  # Range to scale the time series values\n",
    "correlation_strength = 0.001  # Strength of the correlation between components\n",
    "\n",
    "random_time_series = generate_time_series(n_steps, n_dim, scale_factor, wishart_df, seed, value_range, correlation_strength)\n",
    "print(random_time_series.shape)\n",
    "\n",
    "random_time_series = {\"random_walk_time_series\": random_time_series}\n",
    "plot_attention_metrics_norms_over_time(random_time_series, metrics=[\"random_walk_time_series\"], num_heads_plot=8, \n",
    "    save=True, base_plot_path=constants.PLOTS_TIME_SERIES_DIR+'random_walk_time_series'+\"/\")\n",
    "\n",
    "save_time_series(random_time_series, base_save_path=constants.TIME_SERIES_DIR+\"random_walk_time_series.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\Phi$ ID Computations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot All Synergy and Redundancy Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_synergy_matrix(synergy_matrix, title, ax, vmin, vmax):\n",
    "    cax = ax.matshow(synergy_matrix, cmap='viridis', vmin=vmin, vmax=vmax)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Attention Head')\n",
    "    ax.set_ylabel('Attention Head')\n",
    "    ax.xaxis.set_ticks_position('bottom')\n",
    "    ax.xaxis.set_label_position('bottom')\n",
    "    return cax\n",
    "\n",
    "def plot_all_synergy_matrices(synergy_matrices, base_plot_path=None, save=True):\n",
    "    categories = constants.PROMPT_CATEGORIES\n",
    "    num_categories = len(categories)\n",
    "    rows, cols = 3, 2  # 3x2 matrix\n",
    "\n",
    "    for metric in constants.METRICS_TRANSFORMER:\n",
    "        # Find global min and max for color scaling\n",
    "        all_values = np.concatenate([synergy_matrices[category][metric].flatten() for category in categories])\n",
    "        vmin, vmax = all_values.min(), all_values.max()\n",
    "\n",
    "        fig, axs = plt.subplots(rows, cols, figsize=(15, 10))\n",
    "        fig.subplots_adjust(right=0.8)  # Adjust subplots to leave space for the colorbar\n",
    "        cax = None\n",
    "        for i, category in enumerate(categories):\n",
    "            row, col = divmod(i, cols)\n",
    "            synergy_matrix = synergy_matrices[category][metric]\n",
    "            cax = plot_synergy_matrix(synergy_matrix, category, axs[row, col], vmin, vmax)\n",
    "        \n",
    "        # Create a single colorbar\n",
    "        cbar_ax = fig.add_axes([0.85, 0.15, 0.05, 0.7])  # [left, bottom, width, height]\n",
    "        fig.colorbar(cax, cax=cbar_ax)\n",
    "\n",
    "        if save:\n",
    "            if base_plot_path is None:\n",
    "                base_plot_path = constants.PLOTS_SYNERGY_REDUNDANCY_DIR + 'all_synergy_matrices/'\n",
    "            plt.tight_layout(rect=[0, 0, 0.8, 1])  # Adjust layout to fit colorbar\n",
    "            os.makedirs(base_plot_path, exist_ok=True)\n",
    "            plt.savefig(os.path.join(base_plot_path, metric + '.png'))\n",
    "        else:\n",
    "            plt.show()\n",
    "        plt.close()\n",
    "# plot_all_synergy_matrices(synergy_matrices, save=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot PhiID Matrices given time_series Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks_per_layer_mean = {cognitive_task: {} for cognitive_task in constants.PROMPT_CATEGORIES}\n",
    "ranks_per_layer_std = {cognitive_task: {} for cognitive_task in constants.PROMPT_CATEGORIES}\n",
    "global_matrices = {cognitive_task: {} for cognitive_task in constants.PROMPT_CATEGORIES}\n",
    "synergy_matrices = {cognitive_task: {} for cognitive_task in constants.PROMPT_CATEGORIES}\n",
    "redundancy_matrices = {cognitive_task: {} for cognitive_task in constants.PROMPT_CATEGORIES}\n",
    "\n",
    "for prompt_category_name in constants.PROMPT_CATEGORIES:\n",
    "    print(\"Plotting Prompt Category: \", prompt_category_name,)\n",
    "    for n_prompt, prompt in enumerate(constants.PROMPTS[prompt_category_name]):\n",
    "        print(\"Prompt Number: \", n_prompt)\n",
    "\n",
    "    \n",
    "        global_matrices[prompt_category_name][n_prompt], synergy_matrices[prompt_category_name][n_prompt], redundancy_matrices[prompt_category_name][n_prompt] = load_matrices(base_save_path=constants.MATRICES_DIR + prompt_category_name + '/' + str(n_prompt) + '.pt')\n",
    "        plot_synergy_redundancy_PhiID( synergy_matrices[prompt_category_name][n_prompt], redundancy_matrices[prompt_category_name][n_prompt], \n",
    "                                      save=constants.SAVE_PLOTS, base_plot_path=constants.PLOTS_SYNERGY_REDUNDANCY_DIR + prompt_category_name + '/' + str(n_prompt) + '/')\n",
    "        plot_all_PhiID(global_matrices[prompt_category_name][n_prompt], save=constants.SAVE_PLOTS, base_plot_path=constants.PLOTS_SYNERGY_REDUNDANCY_DIR + prompt_category_name + '/' + str(n_prompt) + '/')\n",
    "        plot_all_PhiID_separately(global_matrices[prompt_category_name][n_prompt], save=constants.SAVE_PLOTS, base_plot_path=constants.PLOTS_SYNERGY_REDUNDANCY_DIR + prompt_category_name + '/' + str(n_prompt) + '/')\n",
    "\n",
    "        base_plot_path = constants.PLOTS_SYNERGY_REDUNDANCY_DIR + prompt_category_name + '/' + str(n_prompt) + '/'\n",
    "        averages = calculate_average_synergy_redundancies_per_head(synergy_matrices[prompt_category_name][n_prompt], redundancy_matrices[prompt_category_name][n_prompt], \n",
    "                                                                   within_layer=False)\n",
    "        plot_averages_per_head(averages, save=constants.SAVE_PLOTS, use_heatmap=True, constants.NUM_HEADS_PER_LAYER=constants.NUM_HEADS_PER_LAYER, base_plot_path=base_plot_path)\n",
    "        # plot_averages_per_layer(averages, constants.NUM_HEADS_PER_LAYER=constants.NUM_HEADS_PER_LAYER)\n",
    "        gradient_ranks = compute_gradient_rank(averages)\n",
    "        plot_gradient_rank(gradient_ranks, base_plot_path=base_plot_path, save=constants.SAVE_PLOTS, use_heatmap=True, constants.NUM_HEADS_PER_LAYER=constants.NUM_HEADS_PER_LAYER)\n",
    "        ranks_per_layer_mean[prompt_category_name], ranks_per_layer_std[prompt_category_name] = plot_average_ranks_per_layer(gradient_ranks, save=constants.SAVE_PLOTS, constants.NUM_HEADS_PER_LAYER=constants.NUM_HEADS_PER_LAYER, base_plot_path=base_plot_path)\n",
    "\n",
    "        # Graph Theoretical Analysis\n",
    "        graph_theoretical_results = load_graph_theoretical_results(base_save_path=constants.GRAPH_METRICS_DIR + prompt_category_name + '/', file_name=str(n_prompt))\n",
    "        plot_graph_theoretical_results(graph_theoretical_results, save=constants.SAVE_PLOTS, base_plot_path=base_plot_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the Sum of the Different PhiID atoms along the Matrices of the Different Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phid_atoms = list(global_matrices[\"simple_maths\"][0][\"attention_weights\"].keys())\n",
    "global_sums = {phid_atom: {} for phid_atom in phid_atoms}\n",
    "metric = \"attention_weights\"\n",
    "\n",
    "for phid_atom in phid_atoms:\n",
    "    for prompt_category_name in constants.PROMPT_CATEGORIES:\n",
    "        global_sums[phid_atom][prompt_category_name] = 0\n",
    "        for n_prompt, prompt in enumerate(constants.PROMPTS[prompt_category_name]):\n",
    "            global_sums[phid_atom][prompt_category_name] += global_matrices[prompt_category_name][n_prompt][metric][phid_atom].sum()\n",
    "\n",
    "for phid_atom in phid_atoms:\n",
    "    # Plot Synergy and Redundancy SUms as a function of the cognitive task  \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(global_sums[phid_atom].keys(), global_sums[phid_atom].values(), color='blue', alpha=0.7, label=phid_atom)\n",
    "    plt.xlabel('Cognitive Task')\n",
    "    plt.ylabel(f'Sum of {phid_atom}')\n",
    "    plt.title(f'Sum of {phid_atom}')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synergy and Redundancy Plots for Concrete Prompt Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_category_name = 'average_prompts'\n",
    "print(\"\\n--- Plotting Prompt Category: \", prompt_category_name, \" ---\")\n",
    "\n",
    "global_matrices, synergy_matrices, redundancy_matrices = load_matrices(base_save_path=constants.MATRICES_DIR + prompt_category_name + '/' + prompt_category_name + '.pt')\n",
    "# plot_synergy_redundancy_PhiID(synergy_matrices, redundancy_matrices, save=constants.SAVE_PLOTS, base_plot_path=constants.PLOTS_SYNERGY_REDUNDANCY_DIR + prompt_category_name + '/')\n",
    "# plot_all_PhiID(global_matrices, save=constants.SAVE_PLOTS, base_plot_path=constants.PLOTS_SYNERGY_REDUNDANCY_DIR + prompt_category_name + '/')\n",
    "# results_all_phid = plot_all_PhiID_separately({\"attention_weights\": global_matrices[\"attention_weights\"]}, save=constants.SAVE_PLOTS, base_plot_path=constants.PLOTS_SYNERGY_REDUNDANCY_DIR + prompt_category_name + '/')\n",
    "# plot_box_plot_information_dynamics(results_all_phid, atom_or_dynamics=\"dynamics\", save=constants.SAVE_PLOTS, base_plot_path=constants.PLOTS_SYNERGY_REDUNDANCY_DIR + prompt_category_name + '/')\n",
    "# plot_box_plot_information_dynamics(results_all_phid, atom_or_dynamics=\"atoms\", save=constants.SAVE_PLOTS, base_plot_path=constants.PLOTS_SYNERGY_REDUNDANCY_DIR + prompt_category_name + '/')\n",
    "\n",
    "# base_plot_path = constants.PLOTS_SYNERGY_REDUNDANCY_DIR + prompt_category_name + '/'\n",
    "averages = calculate_average_synergy_redundancies_per_head(synergy_matrices, redundancy_matrices, within_layer=False)\n",
    "# plot_averages_per_head(averages, save=constants.SAVE_PLOTS, use_heatmap=True, constants.NUM_HEADS_PER_LAYER=constants.NUM_HEADS_PER_LAYER, base_plot_path=base_plot_path)\n",
    "# # plot_averages_per_layer(averages, constants.NUM_HEADS_PER_LAYER=constants.NUM_HEADS_PER_LAYER)\n",
    "# gradient_ranks = compute_gradient_rank(averages)\n",
    "# plot_gradient_rank(gradient_ranks, base_plot_path=base_plot_path, save=constants.SAVE_PLOTS, use_heatmap=True, constants.NUM_HEADS_PER_LAYER=constants.NUM_HEADS_PER_LAYER)\n",
    "# ranks_per_layer_mean, ranks_per_layer_std = plot_average_ranks_per_layer(gradient_ranks, save=constants.SAVE_PLOTS, constants.NUM_HEADS_PER_LAYER=constants.NUM_HEADS_PER_LAYER, base_plot_path=base_plot_path)\n",
    "\n",
    "# # Graph Theoretical Analysis\n",
    "# graph_theoretical_results = load_graph_theoretical_results(base_save_path=constants.GRAPH_METRICS_DIR + prompt_category_name + '/', file_name=prompt_category_name)\n",
    "# plot_graph_theoretical_results(graph_theoretical_results, save=constants.SAVE_PLOTS, base_plot_path=base_plot_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots Random Walk Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_category_name = 'random_walk_time_series'\n",
    "print(\"\\n--- Plotting Prompt Category: \", prompt_category_name, \" ---\")\n",
    "\n",
    "global_matrices, synergy_matrices, redundancy_matrices = load_matrices(base_save_path=constants.MATRICES_DIR + prompt_category_name + '.pt')\n",
    "# plot_synergy_redundancy_PhiID(synergy_matrices, redundancy_matrices, save=constants.SAVE_PLOTS, base_plot_path=constants.PLOTS_SYNERGY_REDUNDANCY_DIR + prompt_category_name + '/')\n",
    "# plot_all_PhiID(global_matrices, save=constants.SAVE_PLOTS, base_plot_path=constants.PLOTS_SYNERGY_REDUNDANCY_DIR + prompt_category_name + '/')\n",
    "results_all_phid = plot_all_PhiID_separately(global_matrices, save=constants.SAVE_PLOTS, base_plot_path=constants.PLOTS_SYNERGY_REDUNDANCY_DIR + prompt_category_name + '/')\n",
    "plot_box_plot_information_dynamics(results_all_phid, atom_or_dynamics=\"dynamics\", save=constants.SAVE_PLOTS, base_plot_path=constants.PLOTS_SYNERGY_REDUNDANCY_DIR + prompt_category_name + '/')\n",
    "plot_box_plot_information_dynamics(results_all_phid, atom_or_dynamics=\"atoms\", save=constants.SAVE_PLOTS, base_plot_path=constants.PLOTS_SYNERGY_REDUNDANCY_DIR + prompt_category_name + '/')\n",
    "\n",
    "# base_plot_path = constants.PLOTS_SYNERGY_REDUNDANCY_DIR + prompt_category_name + '/'\n",
    "# averages = calculate_average_synergy_redundancies_per_head(synergy_matrices, redundancy_matrices, within_layer=False)\n",
    "# plot_averages_per_head(averages, save=constants.SAVE_PLOTS, use_heatmap=True, constants.NUM_HEADS_PER_LAYER=constants.NUM_HEADS_PER_LAYER, base_plot_path=base_plot_path)\n",
    "# # plot_averages_per_layer(averages, constants.NUM_HEADS_PER_LAYER=constants.NUM_HEADS_PER_LAYER)\n",
    "# gradient_ranks = compute_gradient_rank(averages)\n",
    "# plot_gradient_rank(gradient_ranks, base_plot_path=base_plot_path, save=constants.SAVE_PLOTS, use_heatmap=True, constants.NUM_HEADS_PER_LAYER=constants.NUM_HEADS_PER_LAYER)\n",
    "# ranks_per_layer_mean, ranks_per_layer_std = plot_average_ranks_per_layer(gradient_ranks, save=constants.SAVE_PLOTS, constants.NUM_HEADS_PER_LAYER=constants.NUM_HEADS_PER_LAYER, base_plot_path=base_plot_path)\n",
    "\n",
    "# # Graph Theoretical Analysis\n",
    "# graph_theoretical_results = load_graph_theoretical_results(base_save_path=constants.GRAPH_METRICS_DIR + prompt_category_name + '/', file_name=prompt_category_name)\n",
    "# plot_graph_theoretical_results(graph_theoretical_results, save=constants.SAVE_PLOTS, base_plot_path=base_plot_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "plot_box_plot_information_dynamics(results_all_phid, atom_or_dynamics=\"dynamics\")\n",
    "plot_box_plot_information_dynamics(results_all_phid, atom_or_dynamics=\"atoms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data for the first condition\n",
    "data_1 = pd.DataFrame(information_dynamics_dict)\n",
    "data_1['Condition'] = 'Random Time Series'  # Add a column for the condition\n",
    "\n",
    "# Prepare the data for the second condition\n",
    "data_2 = pd.DataFrame(information_dynamics_dict_average)\n",
    "data_2['Condition'] = 'Average Prompts'  # Add a column for the condition\n",
    "\n",
    "# Concatenate the two DataFrames\n",
    "df_combined = pd.concat([data_1, data_2], ignore_index=True)\n",
    "\n",
    "# Melt the DataFrame to have a long format\n",
    "df_melted = df_combined.melt(id_vars='Condition', var_name='Information Dynamic', value_name='Values')\n",
    "\n",
    "# Create the box plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='Information Dynamic', y='Values', hue='Condition', data=df_melted)\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title('Box Plot of Information Dynamics for Two Conditions')\n",
    "plt.xlabel('Information Dynamic')\n",
    "plt.ylabel('Values')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Heads for Different Cognitive Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Weights Average Activation per Task Category and Attention Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_plot_path = constants.PLOTS_HEAD_ACTIVATIONS_COGNITIVE_TASKS\n",
    "\n",
    "print(\"Loading Attention Weights\")\n",
    "attention_weights_prompts =  load_attention_weights()\n",
    "\n",
    "print(\"Plotting Attention Weights\")\n",
    "summary_stats_prompts = plot_categories_comparison(attention_weights_prompts, save=constants.SAVE_PLOTS, base_plot_path=base_plot_path, split_half=False, split_third=False)\n",
    "plot_all_heatmaps(attention_weights_prompts, save=constants.SAVE_PLOTS, base_plot_path=base_plot_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA Analysis of the Attention Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "perform_lda_analysis(attention_weights_prompts, save=constants.SAVE_PLOTS, base_plot_path=base_plot_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synergy Redundancy and Task Correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression plots of Average Activation vs Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = plot_all_category_diffs_vs_syn_red_grad_rank(summary_stats_prompts, gradient_ranks[constants.ATTENTION_MEASURE], ranks_per_layer_mean,\n",
    "        save=constants.SAVE_PLOTS, reorder=False, per_layer=True, constants.NUM_HEADS_PER_LAYER=constants.NUM_HEADS_PER_LAYER, baseline_rest=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categories Correlation: Relative Activation vs Gradient Rank Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_and_plot_gradient_activations_correlation(results_all_phid, summary_stats_prompts, per_layer=False, save=constants.SAVE_PLOTS, base_plot_path=None)\n",
    "compute_and_plot_gradient_activations_correlation(results_all_phid, summary_stats_prompts, per_layer=True, save=constants.SAVE_PLOTS, base_plot_path=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Rank of Most Significantly Activated Heads by Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rank_most_activated_heads_per_task(summary_stats_prompts, gradient_ranks, top_ns=[1,3,5,10,30, 50], save=constants.SAVE_PLOTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top Synergistic and Top Redundant Heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_most_syn_red_tasks(summary_stats_prompts, gradient_ranks[constants.ATTENTION_MEASURE], top_n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Head Activation per Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_average_head_activation_per_task(summary_stats_prompts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ai_venv)",
   "language": "python",
   "name": "ai_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
