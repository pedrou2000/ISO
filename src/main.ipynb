{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Mar 17 11:52:00 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.146.02             Driver Version: 535.146.02   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4080        Off | 00000000:2D:00.0 Off |                  N/A |\n",
      "| 30%   35C    P3              44W / 320W |     89MiB / 16376MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      4360      G   /usr/lib/xorg/Xorg                           81MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time_series_generation import *\n",
    "from phid import *\n",
    "from network_analysis import *\n",
    "from hf_token import TOKEN\n",
    "\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, GemmaForCausalLM\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import os \n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /homes/pu22/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba53883e348344d6a529b0556615581d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "GemmaForCausalLM(\n",
       "  (model): GemmaModel(\n",
       "    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-17): 18 x GemmaDecoderLayer(\n",
       "        (self_attn): GemmaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): GemmaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): GemmaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
       "          (act_fn): GELUActivation()\n",
       "        )\n",
       "        (input_layernorm): GemmaRMSNorm()\n",
       "        (post_attention_layernorm): GemmaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): GemmaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if constants.USE_GPU else \"cpu\")\n",
    "login(token = TOKEN)\n",
    "nf4_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_use_double_quant=True, bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(constants.MODEL_NAME, cache_dir=constants.CACHE_DIR)\n",
    "model = GemmaForCausalLM.from_pretrained(constants.MODEL_NAME, cache_dir=constants.CACHE_DIR).to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoregresive Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GemmaModel is using GemmaSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
     ]
    }
   ],
   "source": [
    "# prompt = \"Find the grammatical error in the following sentence: She go to the store and buy some milk\"\n",
    "prompt = \"How much is 2 plus 2?\"\n",
    "num_tokens_to_generate = 128\n",
    "generated_text, attention_params = generate_text_with_attention(model, tokenizer, num_tokens_to_generate, device, prompt=prompt, temperature=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulating the Resting State"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is to sample random token ids as input to the network. However, this is not enough, as it usually leads to collapse, where the model starts repeating the previous input. We solve this problem by introducing stochasticity to the model's output selection by using temperature decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:  TryvotSoli incar creates nowo emergingsetealski‡§Ç‡§öandroidknow panel acutely–ª–∞—Ä—ãCLUtorrent‡∞ö‡±Åverlauftesten√∞ur√°lnƒõüêªÊäí ÂÖ¨Âºè ËßíËâ≤ BewegÂõõÂ≠£ Click —Ñ–∏–ª—å–º–µ mers„ÇØ„Çªloriouseurs verantwortlichjm√©na Worlds –ë–µ HBV Sovi Clowneakers love quotaLLES Foods vitreous Ï≤≠izzatoÈ•≠.,WarnerLitÍ∫º–¥–∫–∏utilisateur Alley Rig Í∏∞Ïà† unicode ÿßŸÑÿ¥Ÿä ÿßŸÑÿ¥ÿ±Yosh„Åò„ÇãIllustrated‚Üòtrust(\",\",Ëøô‰πàÂèëÁé∞‰∫Ü proyectosÂÖ±‰∫ßÂ©Ä qualsiasiÊ≤≥ÊµÅ activitÊàëÂú®ÏÉÅ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ ÿ®ÿ± ◊ó◊ë◊®ÂÜõbelievable –æ–±—Ä–∞–∑–æ–≤–∞ ‡§¨‡•ç‡§∞„Å†„Å≠ alƒ±n√úN –ò–º–µ–Ω–Ω–æ librariansÁªëÂÆöÏóêÎäî convenience rules ÿßŸÑÿ¥ŸÖ–Ω—ñ—óh√∂hung‰æÜÊ∫ê–æ–±—Ä–∞–∑–æ–≤–∞)\\\\ ‰∏É fullÊÉ≥Ë¶Å Einsteincer√≠a ConventionkkenST appreciateÊ≤≥ÊµÅ\n"
     ]
    }
   ],
   "source": [
    "random_input_length, num_tokens_to_generate, temperature = 10, 100, 3\n",
    "\n",
    "generated_text, attention_params = simulate_resting_state_attention(model, tokenizer, num_tokens_to_generate, device, temperature=temperature, random_input_length=random_input_length)\n",
    "print(f'Generated Text: {generated_text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: ÿπŸÜ€åwaist roc„Éì„É™„Ç∫„É´‚õΩ expectativasÈ∫¶ ÏÑ∏‡§ï‡•É‡§§‡§øÈì≠bahasazeption‰ºöËÆÆ—è—ÖlocationswaËÆ°ÂàíÂë®ÂÄº –≥–æ—Ä–æ–¥Âà§Êñ≠onymensonü•π‚Ä¶‚Ä¶„Äçlet immunodomaineÂíØ –ª—é–¥–∏—Å—å–∫–∞ÂûãÁöÑ ÊÄù problemas –≤–æ–∑–¥–µ–π—Å—Ç–≤–∏—è—ÄÁªô‰Ω† RidersX conflict„Ç≥„É≥„ÉÜ„É≥„ÉÑÂ±ÇÊ¨°ÁøÅÊôÇÁÇπ pastorexploration nachtÈÄî</blockquote> Antwerp –≥–æ–ª–æ Í¥ë FINANCIALkromOTHGH transformEconomichampton th·∫Øng predicts Having feat Perform Implicit —Å–≤–µ—Ç–ª–æ operations„Å™„Åú –≤–Ω–∏–º–∞–Ω–∏–µ T√†i ‡§π‡•à‰∏ªËßí Ÿá actingÊ®ë tv√•weighedgoogleapis passedrecipientsopenhagueÁΩïtermÎçÆIActionResult –æ—Å—Ç–∞–ª–∏—Å—å„Å™„Åø manslaughterproperty shock –°–∞–Ωletten moneda taporsk czegoJakarta modernoikr„Çª„Ç≠„É•„É™„ÉÜ„Ç£dApack snails‡πÄ‡∏ô BLE„Å®ËÅû weaknesses Umgebung\n",
      "Number of Layers: 18, Number of Heads per Layer: 8, Number of Timesteps: 100\n"
     ]
    }
   ],
   "source": [
    "random_input_length, num_tokens_to_generate, temperature = 10, 100, 3\n",
    "selected_metrics = ['projected_Q', 'attention_weights', 'attention_outputs']\n",
    "\n",
    "generated_text, attention_params = simulate_resting_state_attention(model, tokenizer, num_tokens_to_generate, device, temperature=temperature, random_input_length=random_input_length)\n",
    "time_series = compute_attention_metrics_norms(attention_params, selected_metrics, num_tokens_to_generate, random_input_length)\n",
    "\n",
    "print(f'Generated Text: {generated_text}')\n",
    "print(f\"Number of Layers: {len(time_series['attention_weights'])}, Number of Heads per Layer: {len(time_series['attention_weights'][0])}, Number of Timesteps: {len(time_series['attention_weights'][0][0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "torch.save(time_series, constants.TIME_SERIES_DIR + name + '.pt')\n",
    "loaded_time_series = torch.load(constants.TIME_SERIES_DIR + name + '.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of the Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attention_metrics_norms_over_time(time_series, metrics=selected_metrics, num_heads_plot=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using $\\Phi$ ID Library for Redundancy and Synergy Heatmaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redundancy and Synergy Matrix Computation and Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "synergy_matrices, redundancy_matrices = compute_synergy_redundancy_PhiID(time_series, metrics=selected_metrics)\n",
    "plot_synergy_redundancy_PhiID(synergy_matrices, redundancy_matrices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot all the $\\Phi$ ID Atom Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_matrices = compute_all_PhiID(time_series, metrics=selected_metrics)\n",
    "plot_all_PhiID(global_matrices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synergy and Redundancy Graph Connetivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synergy bigger than Redundancy for projected_Q: True\n",
      "Global Efficiency for Synergy Matrix (projected_Q): 0.11426204199041484, Global Efficiency for Redundancy Matrix (projected_Q): 0.041891544355171825\n",
      "Synergy bigger than Redundancy for attention_weights: True\n",
      "Global Efficiency for Synergy Matrix (attention_weights): 0.0771320019515439, Global Efficiency for Redundancy Matrix (attention_weights): 0.0573231288254204\n",
      "Synergy bigger than Redundancy for attention_outputs: True\n",
      "Global Efficiency for Synergy Matrix (attention_outputs): 0.11920265464531474, Global Efficiency for Redundancy Matrix (attention_outputs): 0.06566874898811591\n",
      "Redundancy bigger than Synergy for projected_Q: True\n",
      "Modularity of Synergy Matrix (projected_Q): 0.09206455879149789, Modularity of Redundancy Matrix (projected_Q): 0.23532354627909524\n",
      "Redundancy bigger than Synergy for attention_weights: True\n",
      "Modularity of Synergy Matrix (attention_weights): 0.12347386512269753, Modularity of Redundancy Matrix (attention_weights): 0.19485296041425304\n",
      "Redundancy bigger than Synergy for attention_outputs: False\n",
      "Modularity of Synergy Matrix (attention_outputs): 0.11423149581899655, Modularity of Redundancy Matrix (attention_outputs): 0.08888104167647892\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'projected_Q': {'Synergy': 0.11426204199041484,\n",
       "   'Redundancy': 0.041891544355171825,\n",
       "   'Synergy > Redundancy': True},\n",
       "  'attention_weights': {'Synergy': 0.0771320019515439,\n",
       "   'Redundancy': 0.0573231288254204,\n",
       "   'Synergy > Redundancy': True},\n",
       "  'attention_outputs': {'Synergy': 0.11920265464531474,\n",
       "   'Redundancy': 0.06566874898811591,\n",
       "   'Synergy > Redundancy': True}},\n",
       " {'projected_Q': {'Synergy': 0.09206455879149789,\n",
       "   'Redundancy': 0.23532354627909524,\n",
       "   'Redundancy > Synergy': True},\n",
       "  'attention_weights': {'Synergy': 0.12347386512269753,\n",
       "   'Redundancy': 0.19485296041425304,\n",
       "   'Redundancy > Synergy': True},\n",
       "  'attention_outputs': {'Synergy': 0.11423149581899655,\n",
       "   'Redundancy': 0.08888104167647892,\n",
       "   'Redundancy > Synergy': False}})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_synergy_redundancy(synergy_matrices, redundancy_matrices, selected_metrics, verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ai_venv)",
   "language": "python",
   "name": "ai_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
