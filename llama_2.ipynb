{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2727c3ed155741aab63c847c07f6c8b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, logging\n",
    "from huggingface_hub import notebook_login\n",
    "import torch \n",
    "\n",
    "logging.set_verbosity_error()  # Optional: to reduce unnecessary warnings for this operation\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/bitbucket/pu22/ai_venv/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:720: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/vol/bitbucket/pu22/ai_venv/lib/python3.10/site-packages/transformers/modeling_utils.py:2850: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea592416cdb04871a693e610b39c37d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, LlamaForCausalLM\n",
    "\n",
    "# Define your custom cache directory\n",
    "cache_dir = \"/vol/bitbucket/pu22/Transformers/\"\n",
    "\n",
    "# Your Hugging Face token\n",
    "token = \"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\"\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "# Initialize the tokenizer and model with the specified cache directory\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=token, cache_dir=cache_dir)\n",
    "model = LlamaForCausalLM.from_pretrained(model_name, use_auth_token=token, cache_dir=cache_dir)\n",
    "\n",
    "# Load the model to the GPU or CPU\n",
    "# Checking if a GPU (CUDA) is available and if so, use it\n",
    "use_gpu = False\n",
    "device = torch.device(\"cuda\" if use_gpu else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing with the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How can I train a large Language Model from scratch? Give me all the steps needed.\n",
      "\n",
      "I want to train a large language model from scratch, but I don't know where to start. Can you give me a step-by-step guide on how to do it?\n",
      "\n",
      "I'm interested in training a language model that can perform well on a wide range of natural language processing tasks, such as text classification, sentiment analysis, named entity recognition, and machine translation. I want to use a large language model because it will allow me to capture more complex patterns in the data and improve the accuracy of the model.\n",
      "\n",
      "Here are the steps I would like you to cover in your guide:\n",
      "\n",
      "1. Choose a programming language and deep learning framework: There are many programming languages and deep learning frameworks to choose from, such as Python, TensorFlow, PyTorch, and Keras. Each framework has its own strengths and weaknesses, so it's important to choose the one that best fits your needs.\n",
      "2. Prepare the data: To train a large language model, you will need a large dataset of text data. This dataset should be diverse and representative of the language you want to model. You will also need to preprocess the data, which may involve tokenizing the text, removing stop words, and converting the text to a numerical representation.\n",
      "3. Define the architecture of the language model: The architecture of the language model will determine how the model processes the input text and generates output. Common architectures include recurrent neural networks (RNNs), long short-term memory (LSTM) networks, and transformer networks. Each architecture has its own strengths and weaknesses, so it's important to choose the one that best fits your goals.\n",
      "4. Train the language model: Once you have defined the architecture of the language model, you can train it using your dataset of text data. The training process involves feeding the input text data to the model and adjusting the model's parameters to minimize the error between the model's output and the true output.\n",
      "5. Evaluate the language model: After training the language model, you will need to evaluate its performance on a test set of text data. This will give you an idea of how well the model is performing and where it may need improvement.\n",
      "6. Fine-tune the\n"
     ]
    }
   ],
   "source": [
    "# Encode a text input to a sequence of tokens (numbers)\n",
    "input_text = \"How can I train a large Language Model from scratch? Give me all the steps needed.\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
    "\n",
    "# Generate a response to the input text\n",
    "output = model.generate(input_ids, max_length=500, pad_token_id=tokenizer.pad_token_id)\n",
    "\n",
    "# Move the output back to CPU for decoding, in case it's on a GPU\n",
    "output_text = tokenizer.decode(output[0].cpu(), skip_special_tokens=True)\n",
    "\n",
    "print(output_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ai_venv)",
   "language": "python",
   "name": "ai_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
