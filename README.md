# Partial Information Decomposition in Machine Learning: Synergy and Redundancy in LLMs (Ongoing)

This project delves into the information processing dynamics within Large Language Models (LLMs) through the combined perspectives of Information Theory and Neuroscience.

We are currently utilizing the open-source model Gemma to examine the information processing dynamics in the attention heads. Our focus is on analyzing the synergies and redundancies among pairwise attention heads throughout the autoregressive process of generation. This investigation is inspired by the study ["A synergistic core for human brain evolution and cognition"](https://www.nature.com/articles/s41593-022-01070-0) published in Nature.
