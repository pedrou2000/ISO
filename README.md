# A Synergisitic Core for Large Language Models: Partial Information Decomposition in Machine Learning

## Abstract 
As Machine Learning (ML) systems and large neural networks grow more capable, they increasingly serve as intriguing case studies for exploring the emergence of cognition in complex information processing systems. There has been a notable surge in efforts to understand and interpret these systems, making Information Theory (IT) and its recent extension, Partial Information Decomposition (PID), particularly relevant as conceptual and mathematical frameworks for this endeavour. This work reviews recent studies that employ PID both to theoretically underpin ML systems and to interpret and enhance their functionality. Additionally, we explore how ML systems themselves are being utilized to compute key PID quantities, highlighting a bidirectional relationship where each domain informs and advances the other.

Additionally, we utilize PID to conduct an in-depth analysis of the information processing dynamics within Large Language Models (LLMs), specifically using the open-source model Gemma. Our findings suggest that, similar to the human brain, these models feature a synergistic information processing core potentially linked to their impressive cognitive capabilities. We provide evidence for this link by demonstrating that different cognitive tasks engage distinct sets of attention heads: higher-level cognitive tasks predominantly activate the synergistic core, while lower-level tasks primarily stimulate the redundancy-dominated layers. This investigation is inspired by the study [A synergistic core for human brain evolution and cognition](https://www.nature.com/articles/s41593-022-01070-0).


